{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModiSpeeches.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "xRtgH6Efygsz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **               Character Level Language Model to write Speeches in Narendra modi's Style**\n",
        "\n",
        "> In this work LSTM RNN is used as a generative model to generate entirely \n",
        "new plausible sequences of texts in our prime minister Narendra Modi's Style. Generative models are not only used to learn how well the model is trained but also learn more about problem domain itself. Since LSTM RNN takes a long time to train, Google colab is used to train the model on GPU\n"
      ]
    },
    {
      "metadata": {
        "id": "ZSphIx6My1bM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **What is RNN ?**\n",
        "\n",
        "> Recurrent Neural Network (RNN) is a type of Deep Neural Network  where connections between units form a directed graph along a sequence.This allows it to exhibit temporal dynamic behavior for a time sequence.\n",
        "\n",
        "#**Why RNN and Why not other models? **\n",
        "\n",
        "> Since Texts are sequences, unlike feedforward neural networks which doesn't share features learned across differnt positions of text, RNNs can use their internal state (memory) to process sequences of inputs thus making it more suitable.\n",
        "\n",
        "# **What is LSTM ?**\n",
        "\n",
        "> LSTM (Long Short Term Memory. This tracks and updates a \"cell state\" or memory variable $c^{\\langle t \\rangle}$ at every time-step, which can be different from $a^{\\langle t \\rangle}$. LSTM uses three gates namely forget gate, update gate and output gate to keep track the information\n",
        "\n",
        "> **Forget gate**\n",
        "\n",
        ">Lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this\n",
        "\n",
        "> **Update gate**\n",
        "\n",
        "> Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural.\n",
        "\n",
        "> **Output gate**\n",
        "\n",
        "\n",
        "\n",
        "> Output gate is used to decide which outputs we will use.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **Our Steps**\n",
        "\n",
        "\n",
        "> Install Dependencies\n",
        "\n",
        "> Upload our Dataset from Google Drive\n",
        "\n",
        "> Preprocess the Data\n",
        "\n",
        "> Build a Simple LSTM Model\n",
        "\n",
        "> Generate new texts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "auRXXOeG3U3E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Installing and uploading dataset**"
      ]
    },
    {
      "metadata": {
        "id": "0JhYXhpaNxx_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1. Authenticate and create the PyDrive client.\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "losTD9DQ2En-",
        "colab_type": "code",
        "outputId": "739d0cbb-6ec2-4e6a-a76b-a53ee3ea0908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "cell_type": "code",
      "source": [
        "#List the Files in the specified folder in Google Drive\n",
        "\n",
        "file_list = drive.ListFile({'q': \"'1fiRsurTg6CvaL2Dx6Xd-DsEKYhh76Crm' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: ModiSpeeches.ipynb, id: 1lAIwuI0bU-VtXUfZWbMKPsZPtyvcIl3z\n",
            "title: ModiSpeeches.txt, id: 1n28dUjB3W8ZgTly7ndqDYyNi17dV6aOp\n",
            "title: Names.ipynb, id: 1PTdqtiaimr8PU31m4WC_UZHNXyszr_hh\n",
            "title: Copy of Names.ipynb, id: 1iexxHw25QBb0iYYjtErkoGIFVCV5LcHz\n",
            "title: Untitled8.ipynb, id: 10_SIEFLue1728fVlllNuVge3Lh_q-lw9\n",
            "title: Untitled7.ipynb, id: 1lKVaoE34kUgEmqWhv19JraabAZBXkYD6\n",
            "title: Indian-Male-Names.txt, id: 1bH10DOHhTCpR94CYk-eoJIQqvjd6_zHI\n",
            "title: Untitled6.ipynb, id: 1CFxaShMFNeOWZF4tRwReIR-WLrauVWTU\n",
            "title: MNIST.ipynb, id: 1yeMuxeyBB-DbsnOnvHyfTrZQfrhlw4eD\n",
            "title: Indian-Male-Names.csv, id: 166hWSjckniZenXcT5rQ2wOdkLXCDlqo8\n",
            "title: Copy of MNIST.ipynb, id: 10xV6-WUpwwtIrX39iQfIh_KAOAF28zQC\n",
            "title: Copy of Untitled6.ipynb, id: 1G8WDzrJC_aeKQ9IcmPHNsWHZbNzBoLK1\n",
            "title: Untitled0.ipynb, id: 16cEf0E-IZrQcP3ci1BMk0t_sj4hYODu7\n",
            "title: Copy of TGSSalt.ipynb, id: 1-9805fejW2UH_bVyl7e4TSLwdOV04Ny4\n",
            "title: Untitled5.ipynb, id: 13SdL2Bh0OeAz-_vbHCbNiE1gOJbyT-mf\n",
            "title: kaggle.json, id: 1YMElDjD60c2A__qlZZlf0KWT0gn3cgtg\n",
            "title: Untitled4.ipynb, id: 1ebWDqMksUen7uIKmt32PUWgcS_RbylTi\n",
            "title: TGSSalt.ipynb, id: 1gMViKQaO2rte0RvMBY8hBd0IaXcDw_RU\n",
            "title: Untitled3.ipynb, id: 1AfCEkkpW2iIaaE8D0ADSG-plu5841Qyo\n",
            "title: Untitled2.ipynb, id: 1P4K0FXhDJK37ICXOsA0xg6dr1c1qCOw2\n",
            "title: Untitled1.ipynb, id: 1URpumRDyKojU0m8epDtVAviZ91G7TeL2\n",
            "title: Airquality.csv, id: 1hHmETnctxm8q9xtL90u-wvonZKDbp3re\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CNNmfJVcBdRn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#importing numpy\n",
        "import numpy as np\n",
        "\n",
        "#import Keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zSczmCNL3Ccw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Uploading Dataset\n",
        "train_downloaded = drive.CreateFile({'id': '1n28dUjB3W8ZgTly7ndqDYyNi17dV6aOp'})\n",
        "train_downloaded.GetContentFile('ModiSpeeches.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j8tgZFUtE-17",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1.PROBLEM STATEMENT"
      ]
    },
    {
      "metadata": {
        "id": "H1w-uRXu6gtL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **1. 1 Dataset and Preprocessing**"
      ]
    },
    {
      "metadata": {
        "id": "tXzEwBUU6dLs",
        "colab_type": "code",
        "outputId": "b8330dab-e4dc-4e93-aa1c-6644d1575a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Read the text file\n",
        "data = open(\"ModiSpeeches.txt\",'r').read()\n",
        "\n",
        "characterset1 = sorted(list(set(data)))\n",
        "data_size, vocab_size = len(data), len(characterset1)\n",
        "print(\"Total number of characters \",data_size)\n",
        "print(\"Number of uniques characters\",vocab_size)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of characters  1725004\n",
            "Number of uniques characters 162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "273qHXzq7Tgt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "    \n",
        "> We have 162 unique characters, this dataset obviously needs some cleaning. Let us remove punctuations and Duplicated spaces\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_ziziY5h8gJF",
        "colab_type": "code",
        "outputId": "088af5bf-8da1-4824-acc5-a6813d52e6b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import re, string \n",
        "\n",
        "#Keep all the alphabets and spaces remove everything else\n",
        "delete = re.compile('[^a-zA-Z ]')\n",
        "\n",
        "#First parameter is the replacement, second parameter is your input string\n",
        "data=delete.sub(' ', data)\n",
        "\n",
        "#Remove Duplicated Spaces\n",
        "data=re.sub(' +', ' ',data)\n",
        "data = data.lower()\n",
        "\n",
        "characterset = sorted(list(set(data)))\n",
        "data_size, vocab_size = len(data), len(characterset)\n",
        "print(\"Size of the dataset\",data_size)\n",
        "print(\"Number of uniques characters\",vocab_size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the dataset 1670828\n",
            "Number of uniques characters 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u37drW0JrbII",
        "colab_type": "code",
        "outputId": "6a5f89e7-0ae9-45b2-c07f-fafd6b528afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "#Mask each character to a number, making it easy for the LSTM to train\n",
        "char_to_int = dict((c, i) for i, c in enumerate(characterset))\n",
        "print(char_to_int)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hZoX2ARKuB4h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 SIMPLE LSTM MODEL"
      ]
    },
    {
      "metadata": {
        "id": "SlreuN0uEdpc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  ** 2.1 MODEL OVERVIEW**\n",
        ">   At each time step, map T_x sequence of characters to the next character. Here T_x acts as a sliding window. ie allowing each character a chance to be learned from the T_x characters that preceded it (except the first T_x characters).\n",
        "  \n",
        "  \n",
        "> **ARGUMENTS :**\n",
        "\n",
        "> n_x = No of Training examples \n",
        "\n",
        "> T_x = Length of the input sequence, which acts as a sliding window.\n",
        "\n",
        "> X_train = list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
        "\n",
        "> Y_train =  list of integers, exactly the same as X but shifted T_x index to the left.\n",
        "\n",
        "> X = Training set of shape [n_x, T_x, 1]\n",
        "\n",
        "> y = One hot encodings of output pattern.  Each y value is converted into a sparse vector with a length of 27, full of zeros except with a 1 in the column for the letter  that the pattern represents.\n",
        "\n",
        "\n",
        "> **Why do we need One-hot-encoding ?**\n",
        "\n",
        "> Without one-hot encoding we are forcing our RNN to precisely predict the next character, whereas one-hot encoding allows us to use softmax activation function from which we can predict the probability of occurence of all possible 27 characters a more easier representation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UfanR197uA98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "GV9rUOT6r-t9",
        "colab_type": "code",
        "outputId": "90aaacf0-ccef-4754-b79f-f02a0ffc57f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "T_x = 50\n",
        "X_train = []\n",
        "Y_train = []\n",
        "for t in range(0, data_size - T_x, 1):\n",
        "\tinp = data[t:t + T_x]\n",
        "\tout = data[t + T_x]\n",
        "\tX_train.append([char_to_int[char] for char in inp])\n",
        "\tY_train.append(char_to_int[out])\n",
        "n_x = len(X_train)\n",
        "print(\"Training set size \",n_x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set size  1670778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "44n-r8Q6tPwt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(X_train, (n_x, T_x, 1))\n",
        "# normalize\n",
        "X = X / float(vocab_size)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(Y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J7RZ9tmplTio",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **2.2 Defining the LSTM model**\n",
        "\n",
        "\n",
        ">*  Create  an LSTM Model with 256 memory units.   \n",
        "\n",
        ">*  Add  Dropout of probabilty 50 percentage\n",
        "\n",
        ">*  Add a Dense Layer with softmax activation function\n",
        "\n",
        ">*   Since it is a Multiclass classification problem with 27 classes we need to compile categorical_cross entropy loss function\n",
        "\n",
        ">*   Using Adam optimizer - a combination of RMS Prop and momentum, to speed up the algorithm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hgvWYvfet6vE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3gzcGYgQuA6F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the checkpoint\n",
        "filepath=\"parameters-updates-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FBfXyJwsuHpN",
        "colab_type": "code",
        "outputId": "0a145f9f-55c8-4305-f4bc-0b6301ed4c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1027
        }
      },
      "cell_type": "code",
      "source": [
        "modelfit = model.fit(X, y, epochs=12, batch_size=128, callbacks=callbacks_list)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "1670778/1670778 [==============================] - 4704s 3ms/step - loss: 2.5640\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.56403, saving model to parameters-updates-01-2.5640.hdf5\n",
            "Epoch 2/12\n",
            "1670778/1670778 [==============================] - 4654s 3ms/step - loss: 2.2208\n",
            "\n",
            "Epoch 00002: loss improved from 2.56403 to 2.22085, saving model to parameters-updates-02-2.2208.hdf5\n",
            "Epoch 3/12\n",
            " 531200/1670778 [========>.....................] - ETA: 53:00 - loss: 2.0855"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-71b196944f4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "WGVp_-0_0f-j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# GENERATING TEXT WITH LSTM NETWORK\n",
        "\n",
        "\n",
        "\n",
        "> Generating text with LSTM is straightforward we just have to load the parameters from the checkpoint file which has the lowest loss, perform one step of forward propagation instead of feeding the actual input to the next cell we feed the sampled output of the previous cell\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nd2FqM6I2mXE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the network weights\n",
        "filename = \"parameters-updates-02-2.2208.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "24KK0Ezl3bp0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(characterset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ADbPON6Z3ow9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jiOZezN83FS4",
        "colab_type": "code",
        "outputId": "50d8fe68-5a09-4605-cfa1-34cc0b5db40d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(X_train)-1)\n",
        "pattern = X_train[start]\n",
        "print(\"Seed:\")\n",
        "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(vocab_size)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\"  the end of two world wars and our conflict this i \"\n",
            "s\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pZ2BKNuaFnBP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion :\n",
        "\n",
        "> We can see some funny texts some spelling mistakes may be if we train our model  longer we can expect our model to produce great results as theres is always a room for improvement.\n",
        "\n",
        "# Future Works\n",
        "\n",
        "> Build a Larger LSTM Model\n",
        "\n",
        "> Tuning Dropouts and Batch size\n",
        "\n",
        "> Changing the LSTM layers to be “stateful” to maintain state across batches.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}
