{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character-level-language-model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "xRtgH6Efygsz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **               Character Level Language Model using KERAS**\n",
        "\n",
        "> In this work LSTM RNN is used as a generative model to generate entirely \n",
        "new plausible sequences of texts in our Prime Minister Narendra Modi's Style. Generative models are not only used to learn how well the model is trained but also to learn more about problem domain itself. Since LSTM RNN takes a long time to train, Google colab is used to train the model on GPU\n"
      ]
    },
    {
      "metadata": {
        "id": "ZSphIx6My1bM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **What is RNN ?**\n",
        "\n",
        "> Recurrent Neural Network (RNN) is a type of Deep Neural Network  where connections between units form a directed graph along a sequence.This allows it to exhibit temporal dynamic behavior for a time sequence.\n",
        "\n",
        "#**Why RNN and Why not other models? **\n",
        "\n",
        "> Since Texts are sequences, unlike feedforward neural networks which doesn't share features learned across different positions of text, RNNs can use their internal state (memory) to process sequences of inputs thus making it more suitable.\n",
        "\n",
        "# **What is LSTM ?**\n",
        "\n",
        "> LSTM (Long Short Term Memory). This tracks and updates a \"cell state\" or memory variable $c^{\\langle t \\rangle}$ at every time-step, which can be different from $a^{\\langle t \\rangle}$. LSTM uses three gates namely forget gate, update gate and output gate to keep track of the information\n",
        "\n",
        "> **Forget gate**\n",
        "\n",
        ">Lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this\n",
        "\n",
        "> **Update gate**\n",
        "\n",
        "> Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural.\n",
        "\n",
        "> **Output gate**\n",
        "\n",
        "\n",
        "\n",
        "> Output gate is used to decide which outputs we will use.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **Our Steps**\n",
        "\n",
        "\n",
        "> Install Dependencies\n",
        "\n",
        "> Upload our Dataset from Google Drive\n",
        "\n",
        "> Preprocess the Data\n",
        "\n",
        "> Build a Simple LSTM Model\n",
        "\n",
        "> Generate new texts\n",
        "\n",
        "> Build a Larger LSTM Model\n",
        "\n",
        "> Generate new texts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "auRXXOeG3U3E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Installing and uploading dataset**"
      ]
    },
    {
      "metadata": {
        "id": "0JhYXhpaNxx_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1. Authenticate and create the PyDrive client.\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "losTD9DQ2En-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#List the Files in the specified folder in Google Drive\n",
        "\n",
        "file_list = drive.ListFile({'q': \"'1fiRsurTg6CvaL2Dx6Xd-DsEKYhh76Crm' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNNmfJVcBdRn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5c7779d3-14c7-4b42-f8b1-490fa79d391f"
      },
      "cell_type": "code",
      "source": [
        "#importing numpy\n",
        "import numpy as np\n",
        "\n",
        "#import Keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "zSczmCNL3Ccw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Uploading Dataset\n",
        "train_downloaded = drive.CreateFile({'id': '1omHsR2nv3TQiT9UT0Ps-a97_BijUENRc'})\n",
        "train_downloaded.GetContentFile('Speeches.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j8tgZFUtE-17",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1.PROBLEM STATEMENT"
      ]
    },
    {
      "metadata": {
        "id": "H1w-uRXu6gtL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **1. 1 Dataset and Preprocessing**"
      ]
    },
    {
      "metadata": {
        "id": "tXzEwBUU6dLs",
        "colab_type": "code",
        "outputId": "376f5123-335d-4271-96b7-50e150efce10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Read the text file\n",
        "data = open(\"Speeches.txt\",'r').read()\n",
        "\n",
        "characterset1 = sorted(list(set(data)))\n",
        "data_size, vocab_size = len(data), len(characterset1)\n",
        "print(\"Total number of characters \",data_size)\n",
        "print(\"Number of uniques characters\",vocab_size)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of characters  48222\n",
            "Number of uniques characters 78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "273qHXzq7Tgt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "    \n",
        "> We have 78 unique characters, this dataset needs some cleaning. Let us remove punctuations and Duplicated spaces\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_ziziY5h8gJF",
        "colab_type": "code",
        "outputId": "ca402dd6-d19e-4c52-f740-a076ffa46bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import re, string \n",
        "\n",
        "#Keep all the alphabets and spaces remove everything else\n",
        "delete = re.compile('[^a-zA-Z ]')\n",
        "\n",
        "#First parameter is the replacement, second parameter is your input string\n",
        "data=delete.sub(' ', data)\n",
        "\n",
        "#Remove Duplicated Spaces\n",
        "data=re.sub(' +', ' ',data)\n",
        "data = data.lower()\n",
        "\n",
        "characterset = sorted(list(set(data)))\n",
        "data_size, vocab_size = len(data), len(characterset)\n",
        "print(\"Size of the dataset\",data_size)\n",
        "print(\"Number of uniques characters\",vocab_size)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the dataset 46625\n",
            "Number of uniques characters 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u37drW0JrbII",
        "colab_type": "code",
        "outputId": "1cae18bc-32c1-4439-d14a-d7b74d923558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "#Mask each character to a number, making it easy for the LSTM to train\n",
        "char_to_int = dict((c, i) for i, c in enumerate(characterset))\n",
        "print(char_to_int)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hZoX2ARKuB4h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 SIMPLE LSTM MODEL"
      ]
    },
    {
      "metadata": {
        "id": "SlreuN0uEdpc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  ** 2.1 MODEL OVERVIEW**\n",
        ">   At each time step, map T_x sequence of characters to the next character. Here T_x acts as a sliding window. ie allowing each character a chance to be learned from the T_x characters that preceded it (except the first T_x characters).\n",
        "  \n",
        "  \n",
        "> **ARGUMENTS :**\n",
        "\n",
        "> n_x = No of Training examples \n",
        "\n",
        "> T_x = Length of the input sequence, which acts as a sliding window.\n",
        "\n",
        "> X_train = list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
        "\n",
        "> Y_train =  list of integers, exactly the same as X but shifted T_x index to the left.\n",
        "\n",
        "> X = Training set of shape [n_x, T_x, 1]\n",
        "\n",
        "> y = One hot encodings of output pattern.  Each y value is converted into a sparse vector with a length of 27, full of zeros except with a 1 in the column for the letter  that the pattern represents.\n",
        "\n",
        "\n",
        "> **Why do we need One-hot-encoding ?**\n",
        "\n",
        "> Without one-hot encoding we are forcing our RNN to precisely predict the next character, whereas one-hot encoding allows us to use softmax activation function from which we can predict the probability of occurence of all possible 27 characters a more easier representation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UfanR197uA98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "GV9rUOT6r-t9",
        "colab_type": "code",
        "outputId": "8e369205-db48-45aa-a7ee-323e5aaa3280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "T_x = 50\n",
        "X_train = []\n",
        "Y_train = []\n",
        "for t in range(0, data_size - T_x, 1):\n",
        "\tinp = data[t:t + T_x]\n",
        "\tout = data[t + T_x]\n",
        "\tX_train.append([char_to_int[char] for char in inp])\n",
        "\tY_train.append(char_to_int[out])\n",
        "n_x = len(X_train)\n",
        "print(\"Training set size \",n_x)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set size  46575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "44n-r8Q6tPwt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(X_train, (n_x, T_x, 1))\n",
        "# normalize\n",
        "X = X / float(vocab_size)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(Y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J7RZ9tmplTio",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **2.2 Defining the LSTM model**\n",
        "\n",
        "\n",
        ">*  Create  an LSTM Model with 256 memory units.   \n",
        "\n",
        ">*  Add  Dropout of probabilty 50 percentage\n",
        "\n",
        ">*  Add a Dense Layer with softmax activation function\n",
        "\n",
        ">*   Since it is a Multiclass classification problem with 27 classes we need to compile categorical_cross entropy loss function\n",
        "\n",
        ">*   Using Adam optimizer - a combination of RMS Prop and momentum, to speed up the algorithm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hgvWYvfet6vE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3gzcGYgQuA6F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the checkpoint\n",
        "filepath=\"parameters-updates-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FBfXyJwsuHpN",
        "colab_type": "code",
        "outputId": "16993ef2-a211-4049-beaa-e9f7695daafa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "cell_type": "code",
      "source": [
        "modelfit = model.fit(X, y, epochs=12, batch_size=128, callbacks=callbacks_list)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "46575/46575 [==============================] - 127s 3ms/step - loss: 2.8737\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.87369, saving model to parameters-updates-01-2.8737.hdf5\n",
            "Epoch 2/12\n",
            "46575/46575 [==============================] - 126s 3ms/step - loss: 2.8327\n",
            "\n",
            "Epoch 00002: loss improved from 2.87369 to 2.83272, saving model to parameters-updates-02-2.8327.hdf5\n",
            "Epoch 3/12\n",
            "46575/46575 [==============================] - 125s 3ms/step - loss: 2.7798\n",
            "\n",
            "Epoch 00003: loss improved from 2.83272 to 2.77980, saving model to parameters-updates-03-2.7798.hdf5\n",
            "Epoch 4/12\n",
            "46575/46575 [==============================] - 125s 3ms/step - loss: 2.7465\n",
            "\n",
            "Epoch 00004: loss improved from 2.77980 to 2.74651, saving model to parameters-updates-04-2.7465.hdf5\n",
            "Epoch 5/12\n",
            "46575/46575 [==============================] - 125s 3ms/step - loss: 2.7099\n",
            "\n",
            "Epoch 00005: loss improved from 2.74651 to 2.70986, saving model to parameters-updates-05-2.7099.hdf5\n",
            "Epoch 6/12\n",
            "46575/46575 [==============================] - 125s 3ms/step - loss: 2.6883\n",
            "\n",
            "Epoch 00006: loss improved from 2.70986 to 2.68831, saving model to parameters-updates-06-2.6883.hdf5\n",
            "Epoch 7/12\n",
            "46575/46575 [==============================] - 126s 3ms/step - loss: 2.6702\n",
            "\n",
            "Epoch 00007: loss improved from 2.68831 to 2.67023, saving model to parameters-updates-07-2.6702.hdf5\n",
            "Epoch 8/12\n",
            "46575/46575 [==============================] - 125s 3ms/step - loss: 2.6561\n",
            "\n",
            "Epoch 00008: loss improved from 2.67023 to 2.65606, saving model to parameters-updates-08-2.6561.hdf5\n",
            "Epoch 9/12\n",
            "46575/46575 [==============================] - 125s 3ms/step - loss: 2.6382\n",
            "\n",
            "Epoch 00009: loss improved from 2.65606 to 2.63817, saving model to parameters-updates-09-2.6382.hdf5\n",
            "Epoch 10/12\n",
            "46575/46575 [==============================] - 126s 3ms/step - loss: 2.6227\n",
            "\n",
            "Epoch 00010: loss improved from 2.63817 to 2.62267, saving model to parameters-updates-10-2.6227.hdf5\n",
            "Epoch 11/12\n",
            "46575/46575 [==============================] - 125s 3ms/step - loss: 2.6042\n",
            "\n",
            "Epoch 00011: loss improved from 2.62267 to 2.60417, saving model to parameters-updates-11-2.6042.hdf5\n",
            "Epoch 12/12\n",
            "46575/46575 [==============================] - 127s 3ms/step - loss: 2.5898\n",
            "\n",
            "Epoch 00012: loss improved from 2.60417 to 2.58978, saving model to parameters-updates-12-2.5898.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WGVp_-0_0f-j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# GENERATING TEXT WITH LSTM NETWORK\n",
        "\n",
        "\n",
        "\n",
        "> Generating text with LSTM is straightforward we just have to load the parameters from the checkpoint file which has the lowest loss, perform one step of forward propagation instead of feeding the actual input to the next cell we feed the sampled output of the previous cell\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nd2FqM6I2mXE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the network weights\n",
        "filename = \"parameters-updates-12-2.5898.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "24KK0Ezl3bp0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(characterset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ADbPON6Z3ow9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jiOZezN83FS4",
        "colab_type": "code",
        "outputId": "227b5c1b-c4bf-444e-da33-72025a3f63c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(X_train)-1)\n",
        "pattern = X_train[start]\n",
        "print(\"Sample Text:\")\n",
        "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]))\n",
        "# generate characters\n",
        "for i in range(50):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(vocab_size)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Text:\n",
            "\" often there are families in the village who celebr\n",
            "the say that is is a soace of the vay that is is a soace of the vay that is soaces "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p7h6iM8HsbEa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Spelling mistakes and some sentences are repeated. This result is not perfect let us improve the model by building a larger LSTM network"
      ]
    },
    {
      "metadata": {
        "id": "cJv7p3cIUVI5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Building Larger LSTM Network**"
      ]
    },
    {
      "metadata": {
        "id": "rPhZehntbVfs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Improve the quality by building a much larger network with same 256 units but an extra layer with reduced dropout probability "
      ]
    },
    {
      "metadata": {
        "id": "ZWsDzZPLUfr8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbRA7gM8VO2P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "3hM3uYL8VQqb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the checkpoint\n",
        "filepath=\"parameters2-updates-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hWHYLyBGVYfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "3ce76df2-0c75-4650-e179-69494f99b163"
      },
      "cell_type": "code",
      "source": [
        "modelfit = model.fit(X, y, epochs=12, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "46575/46575 [==============================] - 378s 8ms/step - loss: 2.8502\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.85015, saving model to parameters2-updates-01-2.8502.hdf5\n",
            "Epoch 2/12\n",
            "46575/46575 [==============================] - 376s 8ms/step - loss: 2.7408\n",
            "\n",
            "Epoch 00002: loss improved from 2.85015 to 2.74079, saving model to parameters2-updates-02-2.7408.hdf5\n",
            "Epoch 3/12\n",
            "46575/46575 [==============================] - 376s 8ms/step - loss: 2.6258\n",
            "\n",
            "Epoch 00003: loss improved from 2.74079 to 2.62584, saving model to parameters2-updates-03-2.6258.hdf5\n",
            "Epoch 4/12\n",
            "46575/46575 [==============================] - 376s 8ms/step - loss: 2.5109\n",
            "\n",
            "Epoch 00004: loss improved from 2.62584 to 2.51091, saving model to parameters2-updates-04-2.5109.hdf5\n",
            "Epoch 5/12\n",
            "46575/46575 [==============================] - 376s 8ms/step - loss: 2.3920\n",
            "\n",
            "Epoch 00005: loss improved from 2.51091 to 2.39205, saving model to parameters2-updates-05-2.3920.hdf5\n",
            "Epoch 6/12\n",
            "46575/46575 [==============================] - 376s 8ms/step - loss: 2.2805\n",
            "\n",
            "Epoch 00006: loss improved from 2.39205 to 2.28054, saving model to parameters2-updates-06-2.2805.hdf5\n",
            "Epoch 7/12\n",
            "46575/46575 [==============================] - 376s 8ms/step - loss: 2.1774\n",
            "\n",
            "Epoch 00007: loss improved from 2.28054 to 2.17745, saving model to parameters2-updates-07-2.1774.hdf5\n",
            "Epoch 8/12\n",
            "46575/46575 [==============================] - 377s 8ms/step - loss: 2.0748\n",
            "\n",
            "Epoch 00008: loss improved from 2.17745 to 2.07481, saving model to parameters2-updates-08-2.0748.hdf5\n",
            "Epoch 9/12\n",
            "46575/46575 [==============================] - 377s 8ms/step - loss: 1.9923\n",
            "\n",
            "Epoch 00009: loss improved from 2.07481 to 1.99231, saving model to parameters2-updates-09-1.9923.hdf5\n",
            "Epoch 10/12\n",
            "46575/46575 [==============================] - 377s 8ms/step - loss: 1.9116\n",
            "\n",
            "Epoch 00010: loss improved from 1.99231 to 1.91161, saving model to parameters2-updates-10-1.9116.hdf5\n",
            "Epoch 11/12\n",
            "46575/46575 [==============================] - 377s 8ms/step - loss: 1.8428\n",
            "\n",
            "Epoch 00011: loss improved from 1.91161 to 1.84276, saving model to parameters2-updates-11-1.8428.hdf5\n",
            "Epoch 12/12\n",
            "46575/46575 [==============================] - 378s 8ms/step - loss: 1.7781\n",
            "\n",
            "Epoch 00012: loss improved from 1.84276 to 1.77813, saving model to parameters2-updates-12-1.7781.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rdusJdn7vhqL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generating Texts"
      ]
    },
    {
      "metadata": {
        "id": "DZPAXtudoDMd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the network weights\n",
        "filename = \"parameters2-updates-12-1.7781.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GO_JZxDjoWFo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(characterset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "utHku9CTrsJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8c371376-a8a2-4da2-d45f-cb4d3d6cfe36"
      },
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(X_train)-1)\n",
        "pattern = X_train[start]\n",
        "print(\"Sample Text:\")\n",
        "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]))\n",
        "# generate characters\n",
        "for i in range(50):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(vocab_size)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Text:\n",
            "\" seeing a good time namaste my greetings to all those \n",
            "the seadh of the sarliament rian i am sake the sas"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yfhQNZAqtvyM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "cellView": "form",
        "outputId": "b8c1208a-865f-401e-ba2a-cc087bf37c56"
      },
      "cell_type": "code",
      "source": [
        "#@title Output 2\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(X_train)-1)\n",
        "pattern = X_train[start]\n",
        "print(\"Sample Text:\")\n",
        "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]))\n",
        "# generate characters\n",
        "for i in range(50):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(vocab_size)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Text:\n",
            "\"  prakash narayan and his wife prabha devi based on\n",
            " the sarling and the say that is is a soace of the"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fqGDLLkcwwml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "cellView": "form",
        "outputId": "5bf61c36-4aea-4f1d-fd4a-eeeafeb8ee55"
      },
      "cell_type": "code",
      "source": [
        "#@title Output 3\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(X_train)-1)\n",
        "pattern = X_train[start]\n",
        "print(\"Sample Text:\")\n",
        "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]))\n",
        "# generate characters\n",
        "for i in range(50):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(vocab_size)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Text:\n",
            "\" our duty to serve mother india by keeping the coun\n",
            "try te siould be and the say the seople in the sar"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5RA5Bc6IwZ9K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "cellView": "form",
        "outputId": "23378ad4-fd30-405c-eaef-ba191c64c857"
      },
      "cell_type": "code",
      "source": [
        "#@title Output 4\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(X_train)-1)\n",
        "pattern = X_train[start]\n",
        "print(\"Sample Text:\")\n",
        "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]))\n",
        "# generate characters\n",
        "for i in range(50):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(vocab_size)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Text:\n",
            "\" ng live lal bahadur shastri my colleagues from cen\n",
            "tring the saslon i am surength in the sarliament r"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_gaFRkmAtJ1r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  We could see some funny texts but there is definitely an improvement comparing to the smaller LSTM. The way the Neural Network learnt the words \"*Namaste*\" \"*Lal Bahadur Shastri*\" and \"*India*\" is fascinating."
      ]
    },
    {
      "metadata": {
        "id": "pZ2BKNuaFnBP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Performance improvement plans  :\n",
        "\n",
        "\n",
        "\n",
        "> Adding more memory unit\n",
        "\n",
        "> Padding input sequence \n",
        "\n",
        "> Tuning Dropouts and Batch size\n",
        "\n",
        "> Changing the LSTM layers to be “stateful” to maintain state across batches.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "XKHPVcGBy2yD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
